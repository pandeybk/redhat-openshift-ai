{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aec1dd9-b96a-4b5b-a5d3-6abd70102f7b",
   "metadata": {},
   "source": [
    "## Understanding the MNIST Dataset\n",
    "\n",
    "- What is MNIST? 'Modified National Institute of Standards and Technology'\n",
    "- Contents of MNIST dataset: Grayscale images of handwritten digits, each of size 28x28 pixels\n",
    "- Labels in MNIST: Integer values representing the digit in the corresponding image, ranging from 0 to 9\n",
    "\n",
    "## Dataset Statistics\n",
    "- Size of training dataset: 60,000 images\n",
    "- Size of testing dataset: 10,000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "702f9488-c73f-431b-a0bc-158be3124e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "113f9727-135e-4c6d-8b73-fdcba69ea4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47040000\n",
      "60000\n",
      "7840000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(train_images.size)\n",
    "print(train_labels.size)\n",
    "print(test_images.size)\n",
    "print(test_labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9291c12-faf9-4b2d-81de-d0bf88609b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47040000\n",
      "7840000\n"
     ]
    }
   ],
   "source": [
    "print(60000*28*28)\n",
    "print(10000*28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1c81a2b-aabe-4185-9d20-160528dd1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b534229b-f307-48ae-b9b4-cf8c9499d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "499bd04e-e6b2-4f8d-b6f0-8a6b80df1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d87811db-9437-43f1-ac48-4c185b88cbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4bf71a2-284c-4f65-b8e0-0ab25607ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00392157 0.00784314 0.01176471 0.01568628 0.01960784\n",
      " 0.02352941 0.02745098 0.03137255 0.03529412 0.03921569 0.04313726\n",
      " 0.04705882 0.05098039 0.05490196 0.05882353 0.0627451  0.06666667\n",
      " 0.07058824 0.07450981 0.07843138 0.08235294 0.08627451 0.09019608\n",
      " 0.09411765 0.09803922 0.10196079 0.10588235 0.10980392 0.11372549\n",
      " 0.11764706 0.12156863 0.1254902  0.12941177 0.13333334 0.13725491\n",
      " 0.14117648 0.14509805 0.14901961 0.15294118 0.15686275 0.16078432\n",
      " 0.16470589 0.16862746 0.17254902 0.1764706  0.18039216 0.18431373\n",
      " 0.1882353  0.19215687 0.19607843 0.2        0.20392157 0.20784314\n",
      " 0.21176471 0.21568628 0.21960784 0.22352941 0.22745098 0.23137255\n",
      " 0.23529412 0.23921569 0.24313726 0.24705882 0.2509804  0.25490198\n",
      " 0.25882354 0.2627451  0.26666668 0.27058825 0.27450982 0.2784314\n",
      " 0.28235295 0.28627452 0.2901961  0.29411766 0.29803923 0.3019608\n",
      " 0.30588236 0.30980393 0.3137255  0.31764707 0.32156864 0.3254902\n",
      " 0.32941177 0.33333334 0.3372549  0.34117648 0.34509805 0.34901962\n",
      " 0.3529412  0.35686275 0.36078432 0.3647059  0.36862746 0.37254903\n",
      " 0.3764706  0.38039216 0.38431373 0.3882353  0.39215687 0.39607844\n",
      " 0.4        0.40392157 0.40784314 0.4117647  0.41568628 0.41960785\n",
      " 0.42352942 0.42745098 0.43137255 0.43529412 0.4392157  0.44313726\n",
      " 0.44705883 0.4509804  0.45490196 0.45882353 0.4627451  0.46666667\n",
      " 0.47058824 0.4745098  0.47843137 0.48235294 0.4862745  0.49019608\n",
      " 0.49411765 0.49803922 0.5019608  0.5058824  0.50980395 0.5137255\n",
      " 0.5176471  0.52156866 0.5254902  0.5294118  0.53333336 0.5372549\n",
      " 0.5411765  0.54509807 0.54901963 0.5529412  0.5568628  0.56078434\n",
      " 0.5647059  0.5686275  0.57254905 0.5764706  0.5803922  0.58431375\n",
      " 0.5882353  0.5921569  0.59607846 0.6        0.6039216  0.60784316\n",
      " 0.6117647  0.6156863  0.61960787 0.62352943 0.627451   0.6313726\n",
      " 0.63529414 0.6392157  0.6431373  0.64705884 0.6509804  0.654902\n",
      " 0.65882355 0.6627451  0.6666667  0.67058825 0.6745098  0.6784314\n",
      " 0.68235296 0.6862745  0.6901961  0.69411767 0.69803923 0.7019608\n",
      " 0.7058824  0.70980394 0.7137255  0.7176471  0.72156864 0.7254902\n",
      " 0.7294118  0.73333335 0.7372549  0.7411765  0.74509805 0.7490196\n",
      " 0.7529412  0.75686276 0.7607843  0.7647059  0.76862746 0.77254903\n",
      " 0.7764706  0.78039217 0.78431374 0.7882353  0.7921569  0.79607844\n",
      " 0.8        0.8039216  0.80784315 0.8117647  0.8156863  0.81960785\n",
      " 0.8235294  0.827451   0.83137256 0.8352941  0.8392157  0.84313726\n",
      " 0.84705883 0.8509804  0.85490197 0.85882354 0.8627451  0.8666667\n",
      " 0.87058824 0.8745098  0.8784314  0.88235295 0.8862745  0.8901961\n",
      " 0.89411765 0.8980392  0.9019608  0.90588236 0.9098039  0.9137255\n",
      " 0.91764706 0.92156863 0.9254902  0.92941177 0.93333334 0.9372549\n",
      " 0.9411765  0.94509804 0.9490196  0.9529412  0.95686275 0.9607843\n",
      " 0.9647059  0.96862745 0.972549   0.9764706  0.98039216 0.9843137\n",
      " 0.9882353  0.99215686 0.99607843 1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.unique(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46f1afbd-b9b0-4409-9be6-df2c3072f815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.2591 - accuracy: 0.9240\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1051 - accuracy: 0.9689\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0693 - accuracy: 0.9794\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0506 - accuracy: 0.9842\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0372 - accuracy: 0.9892\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0285 - accuracy: 0.9916\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0218 - accuracy: 0.9938\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0165 - accuracy: 0.9954\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0122 - accuracy: 0.9970\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0090 - accuracy: 0.9978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f042f341e80>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6b92041-9bad-4e42-af22-c1c92be5ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.reshape((10000, 28*28))\n",
    "\n",
    "test_images = test_images.astype('float')/ 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "661a7811-2da5-402d-af92-adee05e0b907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0660 - accuracy: 0.9808\n",
      "test_loss: 0.06602343916893005\n",
      "test_accuracy: 0.9807999730110168\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"test_loss:\", test_loss)\n",
    "print(\"test_accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f773a679-6f0e-435d-8f46-788ae51c0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/mnist_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26bf96cb-23ee-4eed-8b74-bca86fe4a138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 170ms/step\n",
      "File: data/zero.JPG, Predicted Class: 0\n",
      "File: data/two.JPG, Predicted Class: 3\n",
      "File: data/one.JPG, Predicted Class: 3\n",
      "File: data/nine.JPG, Predicted Class: 3\n",
      "File: data/five.JPG, Predicted Class: 5\n",
      "File: data/4.JPG, Predicted Class: 4\n",
      "File: data/1.JPG, Predicted Class: 8\n",
      "File: data/5.JPG, Predicted Class: 5\n",
      "File: data/eight.JPG, Predicted Class: 8\n",
      "File: data/six.JPG, Predicted Class: 6\n",
      "File: data/three.JPG, Predicted Class: 3\n",
      "File: data/8.JPG, Predicted Class: 8\n",
      "File: data/2.JPG, Predicted Class: 3\n",
      "File: data/four.JPG, Predicted Class: 4\n",
      "File: data/7.JPG, Predicted Class: 3\n",
      "File: data/6.JPG, Predicted Class: 6\n",
      "File: data/3.JPG, Predicted Class: 3\n",
      "File: data/9.JPG, Predicted Class: 3\n",
      "File: data/0.JPG, Predicted Class: 0\n",
      "File: data/seven.JPG, Predicted Class: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import glob\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model('model/mnist_v1.h5')\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(28, 28), color_mode=\"grayscale\")\n",
    "    img_array = np.array(img).reshape(-1, 28 * 28) \n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "    return img_array\n",
    "\n",
    "\n",
    "# Preprocess all the images\n",
    "image_files = glob.glob('data/*.JPG')  \n",
    "images = np.vstack([preprocess_image(img_file) for img_file in image_files])\n",
    "\n",
    "predictions = model.predict(images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the filename and corresponding predicted class for each image\n",
    "for img_file, pred_class in zip(image_files, predicted_classes):\n",
    "    print(f\"File: {img_file}, Predicted Class: {pred_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb19e5-839b-4f84-b753-1eaeeedf7220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
