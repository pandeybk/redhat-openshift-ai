{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1079a2d-9e1a-4ab9-84de-efa7c27ec2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu')) \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation using ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "datagen.fit(train_images)\n",
    "\n",
    "# Train the model\n",
    "model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
    "          steps_per_epoch=len(train_images) // 32, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "model.save('model/mnist_v5-2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3254253c-422f-4cae-8d42-0e235d2cd02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:29:22.115451: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-23 16:29:22.132834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-23 16:29:22.152015: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-23 16:29:22.157231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-23 16:29:22.173831: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-23 16:29:23.487220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/app-root/lib64/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-03-23 16:29:25.191445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 556 MB memory:  -> device: 0, name: NVIDIA A30 MIG 2g.12gb, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742747366.387740     708 service.cc:146] XLA service 0x7f18100164b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742747366.387840     708 service.cc:154]   StreamExecutor device (0): NVIDIA A30 MIG 2g.12gb, Compute Capability 8.0\n",
      "2025-03-23 16:29:26.403597: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-23 16:29:26.457216: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1742747367.445110     708 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "File: data/0.JPG, Predicted Class: 0\n",
      "File: data/1.JPG, Predicted Class: 1\n",
      "File: data/2.JPG, Predicted Class: 2\n",
      "File: data/3.JPG, Predicted Class: 3\n",
      "File: data/4.JPG, Predicted Class: 4\n",
      "File: data/5.JPG, Predicted Class: 5\n",
      "File: data/6.JPG, Predicted Class: 6\n",
      "File: data/7.JPG, Predicted Class: 7\n",
      "File: data/8.JPG, Predicted Class: 8\n",
      "File: data/9.JPG, Predicted Class: 9\n",
      "File: data/eight.JPG, Predicted Class: 8\n",
      "File: data/five.JPG, Predicted Class: 5\n",
      "File: data/four.JPG, Predicted Class: 4\n",
      "File: data/nine.JPG, Predicted Class: 9\n",
      "File: data/one.JPG, Predicted Class: 1\n",
      "File: data/seven.JPG, Predicted Class: 7\n",
      "File: data/six.JPG, Predicted Class: 6\n",
      "File: data/three.JPG, Predicted Class: 3\n",
      "File: data/two.JPG, Predicted Class: 2\n",
      "File: data/zero.JPG, Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import glob\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model('model/mnist_v5.h5')\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(28, 28), color_mode=\"grayscale\")\n",
    "    img_array = np.array(img).reshape(-1, 28, 28, 1) \n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "    return img_array\n",
    "\n",
    "\n",
    "# Preprocess all the images\n",
    "image_files = glob.glob('data/*.JPG')  \n",
    "images = np.vstack([preprocess_image(img_file) for img_file in image_files])\n",
    "\n",
    "predictions = model.predict(images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the filename and corresponding predicted class for each image\n",
    "for img_file, pred_class in zip(image_files, predicted_classes):\n",
    "    print(f\"File: {img_file}, Predicted Class: {pred_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d401fd51-3b8b-49bc-ad75-eea05a0f08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf2onnx in /opt/app-root/lib64/python3.11/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (1.26.4)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (1.17.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (2.32.3)\n",
      "Requirement already satisfied: six in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (24.3.25)\n",
      "Requirement already satisfied: protobuf~=3.20 in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (3.20.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->tf2onnx) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->tf2onnx) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->tf2onnx) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->tf2onnx) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tf2onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b61af6-2c3a-43aa-954b-a07a925ded22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/mnist_v5_saved/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/mnist_v5_saved/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF SavedModel exported to: model/mnist_v5_saved\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1) Load the .h5 model (disable compile if you only need inference)\n",
    "model = tf.keras.models.load_model(\"model/mnist_v5.h5\", compile=False)\n",
    "\n",
    "# 2) Save it as a standard TensorFlow SavedModel\n",
    "tf.saved_model.save(model, \"model/mnist_v5_saved\")\n",
    "\n",
    "print(\"TF SavedModel exported to: model/mnist_v5_saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f845cbbd-198e-4cf2-9009-c27834af1147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "2025-03-23 16:46:48,117 - WARNING - ***IMPORTANT*** Installed protobuf is not cpp accelerated. Conversion will be extremely slow. See https://github.com/onnx/tensorflow-onnx/issues/1557\n",
      "2025-03-23 16:46:48,402 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2025-03-23 16:46:48,626 - INFO - Signatures found in model: [serving_default].\n",
      "2025-03-23 16:46:48,627 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2025-03-23 16:46:48,627 - INFO - Output names: ['output_0']\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742748408.641973    1076 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1742748408.866797    1076 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2025-03-23 16:46:48,920 - INFO - Using tensorflow=2.17.1, onnx=1.17.0, tf2onnx=1.16.1/15c810\n",
      "2025-03-23 16:46:48,920 - INFO - Using opset <onnx, 13>\n",
      "2025-03-23 16:46:48,935 - INFO - Computed 0 values for constant folding\n",
      "2025-03-23 16:46:48,958 - INFO - Optimizing ONNX model\n",
      "2025-03-23 16:46:49,022 - INFO - After optimization: Add -2 (4->2), Cast -1 (1->0), Const +1 (9->10), Identity -2 (2->0), Reshape +1 (1->2), Transpose -7 (8->1)\n",
      "2025-03-23 16:46:49,033 - INFO - \n",
      "2025-03-23 16:46:49,033 - INFO - Successfully converted TensorFlow model model/mnist_v5_saved to ONNX\n",
      "2025-03-23 16:46:49,033 - INFO - Model inputs: ['inputs']\n",
      "2025-03-23 16:46:49,033 - INFO - Model outputs: ['output_0']\n",
      "2025-03-23 16:46:49,033 - INFO - ONNX model is saved at model/mnist_v5.onnx\n"
     ]
    }
   ],
   "source": [
    "# Convert the SavedModel to ONNX\n",
    "!python -m tf2onnx.convert \\\n",
    "    --saved-model model/mnist_v5_saved \\\n",
    "    --output model/mnist_v5.onnx \\\n",
    "    --opset 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd6d781-58b1-4fd9-b39b-b9dbeb1038f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mnist-netsentinel.apps.cloud.xtoph152.dfw.ocp.run'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/0.JPG, Predicted Class: 0\n",
      "File: data/1.JPG, Predicted Class: 1\n",
      "File: data/2.JPG, Predicted Class: 2\n",
      "File: data/3.JPG, Predicted Class: 3\n",
      "File: data/4.JPG, Predicted Class: 4\n",
      "File: data/5.JPG, Predicted Class: 5\n",
      "File: data/6.JPG, Predicted Class: 6\n",
      "File: data/7.JPG, Predicted Class: 7\n",
      "File: data/8.JPG, Predicted Class: 8\n",
      "File: data/9.JPG, Predicted Class: 9\n",
      "File: data/eight.JPG, Predicted Class: 8\n",
      "File: data/five.JPG, Predicted Class: 5\n",
      "File: data/four.JPG, Predicted Class: 4\n",
      "File: data/nine.JPG, Predicted Class: 9\n",
      "File: data/one.JPG, Predicted Class: 1\n",
      "File: data/seven.JPG, Predicted Class: 7\n",
      "File: data/six.JPG, Predicted Class: 6\n",
      "File: data/three.JPG, Predicted Class: 3\n",
      "File: data/two.JPG, Predicted Class: 2\n",
      "File: data/zero.JPG, Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example Python script that:\n",
    "1) Loops through all images in data/*.JPG\n",
    "2) Preprocesses each image (28×28 grayscale, normalized)\n",
    "3) Sends them all in a single batch request to your OpenVINO Model Server endpoint\n",
    "4) Prints each filename with its predicted class\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Your OVMS endpoint URL\n",
    "ENDPOINT_URL = \"https://mnist-netsentinel.apps.cloud.xtoph152.dfw.ocp.run/v2/models/mnist/infer\"\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Convert to 28x28 grayscale, normalize to [0..1], and reshape\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    img = img.resize((28, 28))\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = arr.reshape(1, 28, 28, 1)\n",
    "    return arr\n",
    "\n",
    "def main():\n",
    "    # Collect all JPG images\n",
    "    image_files = glob.glob(\"data/*.JPG\")\n",
    "    if not image_files:\n",
    "        print(\"No .JPG files found in data/ folder.\")\n",
    "        return\n",
    "\n",
    "    # Preprocess each image and stack them\n",
    "    all_images = [preprocess_image(img_file) for img_file in image_files]\n",
    "    images = np.vstack(all_images)  # shape: (N, 28, 28, 1)\n",
    "    batch_size = images.shape[0]\n",
    "\n",
    "    # Flatten the pixel data to a Python list\n",
    "    data_list = images.flatten().tolist()\n",
    "\n",
    "    # Create the inference request payload (KFServing V2 protocol)\n",
    "    # Adjust \"name\" if your model’s input tensor is named differently\n",
    "    request_payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"inputs\",\n",
    "                \"shape\": [batch_size, 28, 28, 1],\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": data_list\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Send POST request to OVMS\n",
    "    response = requests.post(ENDPOINT_URL, json=request_payload, verify=False)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Request failed with status code {response.status_code}\\nResponse: {response.text}\")\n",
    "        return\n",
    "\n",
    "    # Parse the response JSON\n",
    "    resp_json = response.json()\n",
    "    # Typically the output shape will be (N, 10) for MNIST\n",
    "    output_data = resp_json[\"outputs\"][0][\"data\"]\n",
    "    output_array = np.array(output_data).reshape(batch_size, -1)\n",
    "\n",
    "    # Argmax over the last dimension to get the digit classes\n",
    "    predicted_classes = np.argmax(output_array, axis=1)\n",
    "\n",
    "    # Print each filename alongside its predicted class\n",
    "    for img_file, pred_class in zip(image_files, predicted_classes):\n",
    "        print(f\"File: {img_file}, Predicted Class: {pred_class}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
