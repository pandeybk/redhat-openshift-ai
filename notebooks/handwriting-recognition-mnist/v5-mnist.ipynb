{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1079a2d-9e1a-4ab9-84de-efa7c27ec2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.8296 - loss: 0.5183\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 19:59:58.053459: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-03-23 19:59:58.053644: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-03-23 19:59:58.053686: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14413335052435860829\n",
      "2025-03-23 19:59:58.053744: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9918377167445179100\n",
      "/usr/lib64/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9717 - loss: 0.0918\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 20:00:17.199566: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-03-23 20:00:17.199730: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14413335052435860829\n",
      "2025-03-23 20:00:17.199798: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9918377167445179100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9803 - loss: 0.0673\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 20:00:37.003473: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14413335052435860829\n",
      "2025-03-23 20:00:37.003658: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9918377167445179100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9827 - loss: 0.0540\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 20:00:57.241062: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-03-23 20:00:57.241174: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14413335052435860829\n",
      "2025-03-23 20:00:57.241216: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9918377167445179100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.9846 - loss: 0.0494\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 20:01:16.769696: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14413335052435860829\n",
      "2025-03-23 20:01:16.769906: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9918377167445179100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0360\n",
      "Test Loss: 0.02906927838921547\n",
      "Test Accuracy: 0.9911999702453613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu')) \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation using ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "datagen.fit(train_images)\n",
    "\n",
    "# Train the model\n",
    "model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
    "          steps_per_epoch=len(train_images) // 32, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "model.save('model/mnist_v5-2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3254253c-422f-4cae-8d42-0e235d2cd02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n",
      "File: data/0.JPG, Predicted Class: 0\n",
      "File: data/1.JPG, Predicted Class: 1\n",
      "File: data/2.JPG, Predicted Class: 2\n",
      "File: data/3.JPG, Predicted Class: 3\n",
      "File: data/4.JPG, Predicted Class: 4\n",
      "File: data/5.JPG, Predicted Class: 5\n",
      "File: data/6.JPG, Predicted Class: 6\n",
      "File: data/7.JPG, Predicted Class: 7\n",
      "File: data/8.JPG, Predicted Class: 8\n",
      "File: data/9.JPG, Predicted Class: 9\n",
      "File: data/eight.JPG, Predicted Class: 8\n",
      "File: data/five.JPG, Predicted Class: 5\n",
      "File: data/four.JPG, Predicted Class: 4\n",
      "File: data/nine.JPG, Predicted Class: 9\n",
      "File: data/one.JPG, Predicted Class: 1\n",
      "File: data/seven.JPG, Predicted Class: 7\n",
      "File: data/six.JPG, Predicted Class: 6\n",
      "File: data/three.JPG, Predicted Class: 3\n",
      "File: data/two.JPG, Predicted Class: 2\n",
      "File: data/zero.JPG, Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import glob\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model('model/mnist_v5-2.h5')\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(28, 28), color_mode=\"grayscale\")\n",
    "    img_array = np.array(img).reshape(-1, 28, 28, 1) \n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "    return img_array\n",
    "\n",
    "\n",
    "# Preprocess all the images\n",
    "image_files = glob.glob('data/*.JPG')  \n",
    "images = np.vstack([preprocess_image(img_file) for img_file in image_files])\n",
    "\n",
    "predictions = model.predict(images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the filename and corresponding predicted class for each image\n",
    "for img_file, pred_class in zip(image_files, predicted_classes):\n",
    "    print(f\"File: {img_file}, Predicted Class: {pred_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d401fd51-3b8b-49bc-ad75-eea05a0f08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf2onnx in /opt/app-root/lib64/python3.11/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (1.26.4)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (1.17.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (2.32.3)\n",
      "Requirement already satisfied: six in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (24.3.25)\n",
      "Requirement already satisfied: protobuf~=3.20 in /opt/app-root/lib64/python3.11/site-packages (from tf2onnx) (3.20.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->tf2onnx) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->tf2onnx) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->tf2onnx) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->tf2onnx) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tf2onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b61af6-2c3a-43aa-954b-a07a925ded22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/mnist_v5_saved-2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/mnist_v5_saved-2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF SavedModel exported to: model/mnist_v5_saved-2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1) Load the .h5 model (disable compile if you only need inference)\n",
    "model = tf.keras.models.load_model(\"model/mnist_v5-2.h5\", compile=False)\n",
    "\n",
    "# 2) Save it as a standard TensorFlow SavedModel\n",
    "tf.saved_model.save(model, \"model/mnist_v5_saved-2\")\n",
    "\n",
    "print(\"TF SavedModel exported to: model/mnist_v5_saved-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f845cbbd-198e-4cf2-9009-c27834af1147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 20:05:26.180781: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-23 20:05:26.196796: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-23 20:05:26.215177: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-23 20:05:26.220489: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-23 20:05:26.233312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-23 20:05:27.484043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "<frozen runpy>:128: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "2025-03-23 20:05:29,622 - WARNING - ***IMPORTANT*** Installed protobuf is not cpp accelerated. Conversion will be extremely slow. See https://github.com/onnx/tensorflow-onnx/issues/1557\n",
      "2025-03-23 20:05:29.869648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 110 MB memory:  -> device: 0, name: NVIDIA A30 MIG 1g.6gb, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2025-03-23 20:05:29,886 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2025-03-23 20:05:30,116 - INFO - Signatures found in model: [serving_default].\n",
      "2025-03-23 20:05:30,116 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2025-03-23 20:05:30,116 - INFO - Output names: ['output_0']\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742760330.145239    6153 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2025-03-23 20:05:30.145571: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2025-03-23 20:05:30.153209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 110 MB memory:  -> device: 0, name: NVIDIA A30 MIG 1g.6gb, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2025-03-23 20:05:30.294982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 110 MB memory:  -> device: 0, name: NVIDIA A30 MIG 1g.6gb, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "I0000 00:00:1742760330.329747    6153 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2025-03-23 20:05:30.329978: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2025-03-23 20:05:30.337381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 110 MB memory:  -> device: 0, name: NVIDIA A30 MIG 1g.6gb, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2025-03-23 20:05:30,384 - INFO - Using tensorflow=2.17.1, onnx=1.17.0, tf2onnx=1.16.1/15c810\n",
      "2025-03-23 20:05:30,384 - INFO - Using opset <onnx, 13>\n",
      "2025-03-23 20:05:30,398 - INFO - Computed 0 values for constant folding\n",
      "2025-03-23 20:05:30,421 - INFO - Optimizing ONNX model\n",
      "2025-03-23 20:05:30,478 - INFO - After optimization: Add -2 (4->2), Cast -1 (1->0), Const +1 (9->10), Identity -2 (2->0), Reshape +1 (1->2), Transpose -7 (8->1)\n",
      "2025-03-23 20:05:30,487 - INFO - \n",
      "2025-03-23 20:05:30,487 - INFO - Successfully converted TensorFlow model model/mnist_v5_saved to ONNX\n",
      "2025-03-23 20:05:30,487 - INFO - Model inputs: ['inputs']\n",
      "2025-03-23 20:05:30,487 - INFO - Model outputs: ['output_0']\n",
      "2025-03-23 20:05:30,487 - INFO - ONNX model is saved at model/mnist_v5.onnx\n"
     ]
    }
   ],
   "source": [
    "# Convert the SavedModel to ONNX\n",
    "!python -m tf2onnx.convert \\\n",
    "    --saved-model model/mnist_v5_saved \\\n",
    "    --output model/mnist_v5.onnx \\\n",
    "    --opset 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd6d781-58b1-4fd9-b39b-b9dbeb1038f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mnist-v2-netsentinel.apps.cloud.xtoph152.dfw.ocp.run'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/0.JPG, Predicted Class: 0\n",
      "File: data/1.JPG, Predicted Class: 1\n",
      "File: data/2.JPG, Predicted Class: 2\n",
      "File: data/3.JPG, Predicted Class: 3\n",
      "File: data/4.JPG, Predicted Class: 4\n",
      "File: data/5.JPG, Predicted Class: 5\n",
      "File: data/6.JPG, Predicted Class: 6\n",
      "File: data/7.JPG, Predicted Class: 7\n",
      "File: data/8.JPG, Predicted Class: 8\n",
      "File: data/9.JPG, Predicted Class: 9\n",
      "File: data/eight.JPG, Predicted Class: 8\n",
      "File: data/five.JPG, Predicted Class: 5\n",
      "File: data/four.JPG, Predicted Class: 4\n",
      "File: data/nine.JPG, Predicted Class: 9\n",
      "File: data/one.JPG, Predicted Class: 1\n",
      "File: data/seven.JPG, Predicted Class: 7\n",
      "File: data/six.JPG, Predicted Class: 6\n",
      "File: data/three.JPG, Predicted Class: 3\n",
      "File: data/two.JPG, Predicted Class: 2\n",
      "File: data/zero.JPG, Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example Python script that:\n",
    "1) Loops through all images in data/*.JPG\n",
    "2) Preprocesses each image (28×28 grayscale, normalized)\n",
    "3) Sends them all in a single batch request to your OpenVINO Model Server endpoint\n",
    "4) Prints each filename with its predicted class\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Your OVMS endpoint URL\n",
    "ENDPOINT_URL = \"https://mnist-v2-netsentinel.apps.cloud.xtoph152.dfw.ocp.run/v2/models/mnist-v2/infer\"\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Convert to 28x28 grayscale, normalize to [0..1], and reshape\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    img = img.resize((28, 28))\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = arr.reshape(1, 28, 28, 1)\n",
    "    return arr\n",
    "\n",
    "def main():\n",
    "    # Collect all JPG images\n",
    "    image_files = glob.glob(\"data/*.JPG\")\n",
    "    if not image_files:\n",
    "        print(\"No .JPG files found in data/ folder.\")\n",
    "        return\n",
    "\n",
    "    # Preprocess each image and stack them\n",
    "    all_images = [preprocess_image(img_file) for img_file in image_files]\n",
    "    images = np.vstack(all_images)  # shape: (N, 28, 28, 1)\n",
    "    batch_size = images.shape[0]\n",
    "\n",
    "    # Flatten the pixel data to a Python list\n",
    "    data_list = images.flatten().tolist()\n",
    "\n",
    "    # Create the inference request payload (KFServing V2 protocol)\n",
    "    # Adjust \"name\" if your model’s input tensor is named differently\n",
    "    request_payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"inputs\",\n",
    "                \"shape\": [batch_size, 28, 28, 1],\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": data_list\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Send POST request to OVMS\n",
    "    response = requests.post(ENDPOINT_URL, json=request_payload, verify=False)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Request failed with status code {response.status_code}\\nResponse: {response.text}\")\n",
    "        return\n",
    "\n",
    "    # Parse the response JSON\n",
    "    resp_json = response.json()\n",
    "    # Typically the output shape will be (N, 10) for MNIST\n",
    "    output_data = resp_json[\"outputs\"][0][\"data\"]\n",
    "    output_array = np.array(output_data).reshape(batch_size, -1)\n",
    "\n",
    "    # Argmax over the last dimension to get the digit classes\n",
    "    predicted_classes = np.argmax(output_array, axis=1)\n",
    "\n",
    "    # Print each filename alongside its predicted class\n",
    "    for img_file, pred_class in zip(image_files, predicted_classes):\n",
    "        print(f\"File: {img_file}, Predicted Class: {pred_class}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3ffdb-a071-4e3c-a65d-2848423bea63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
